{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h3> We used Google Colab to train our data, so this step is necessery.</h3>\n","<h3> If you use this notebook in local, you can pass this cell.</h3>\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15906,"status":"ok","timestamp":1715061445621,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"b850OUfEi2pi","outputId":"55b3f45a-2167-4cbc-e3db-a2a53a7cfd40"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/YZV405E_NLP/project\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/YZV405E_NLP/project"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Reading Files</b></h1>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coCUb2DkiyK5"},"outputs":[],"source":["#Reading the training files\n","with open('data/wrong.txt', 'r', encoding='utf-8') as file:\n","    wrong_sentences = file.readlines()\n","\n","with open('data/correct.txt', 'r', encoding='utf-8') as file:\n","    correct_sentences = file.readlines()\n","\n","#Getting sentences\n","wrong_sentences = [sentence.strip() for sentence in wrong_sentences]\n","correct_sentences = [sentence.strip() for sentence in correct_sentences]\n","\n","train_x = wrong_sentences  #Wrong sentences (without Turkish characters) as train_x\n","train_y = correct_sentences  #Correct sentences as train_y"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Tokenization</b></h1>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kF22_vvrBrx_"},"outputs":[],"source":["MAX_LENGTH = 500  #Limiting the maximum sequence length\n","\n","#Sacrificing some sequences to minimize the required computational power, we have still over 99% of thr training set remaining!\n","train_x_filtered = [sentence for sentence in train_x if len(sentence) <= MAX_LENGTH]\n","train_y_filtered = [sentence for sentence in train_y if len(sentence) <= MAX_LENGTH]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"som7Rny7iyK7"},"outputs":[],"source":["#Lists to store tokenized sequences\n","tokenized_train_x = []\n","tokenized_train_y = []\n","\n","for sentence in train_x_filtered:\n","    tokenized_sentence = list(sentence)  #Tokenizing into individual characters\n","    tokenized_sentence.append('<EOS>')   #Adding <EOS> token to mark end of sequences\n","    tokenized_train_x.append(tokenized_sentence)\n","\n","for label in train_y_filtered:\n","    tokenized_label = list(label)\n","    tokenized_label.append('<EOS>')\n","    tokenized_train_y.append(tokenized_label)"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Padding</b></h1>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdmWVP38aTy_"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","desired_length = 501 #500 + 1, adding 1 because of the <EOS> token.\n","\n","PAD_TOKEN = '<PAD>'  #Padding token\n","\n","#Padding sequences\n","padded_train_x = pad_sequences(tokenized_train_x, maxlen=desired_length, padding='post', truncating='post', value=PAD_TOKEN, dtype=object)\n","padded_train_y = pad_sequences(tokenized_train_y, maxlen=desired_length, padding='post', truncating='post', value=PAD_TOKEN, dtype=object)\n","\n","#Converting padding sequences back to lists\n","padded_train_x = [list(seq) for seq in padded_train_x]\n","padded_train_y = [list(seq) for seq in padded_train_y]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cccn_fAmsTqO"},"outputs":[],"source":["#Flattening the list of lists in padded_train_x\n","flattened_padded_train_x = [char for sequence in padded_train_x for char in sequence]\n","\n","unique_characters = set(flattened_padded_train_x) #Getting unique characters from padded_train_x\n","turkish_characters = [\"ı\", \"ö\", \"ş\", \"ğ\", \"ü\", \"ç\"]\n","unique_characters.update(turkish_characters) #Adding Turkish unique characters\n","\n","#Creating dictionaries to use in embedding, each token will be represented by its index number\n","index_to_char = {index: char for index, char in enumerate(unique_characters)}\n","char_to_index = {char: index for index, char in enumerate(unique_characters)}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1715061478120,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"sFJnPf7PCz6t","outputId":"a6447961-4df0-42cf-a974-44cdc6ec116d"},"outputs":[{"data":{"text/plain":["dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["index_to_char.keys() #Shows the number of unique tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XmBKEUJ7rya"},"outputs":[],"source":["#Embedding train_x and train_y\n","numerical_train_x = [[char_to_index[char] for char in seq] for seq in padded_train_x]\n","numerical_train_y = [[char_to_index[char] for char in seq] for seq in padded_train_y]"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Model Design</b></h1>\n","<h2>Parameters setting</h2>\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715067056316,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"-dLYWgk0iyK8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchsummary import summary\n","\n","vocab_size = len(unique_characters) #Number of unique tokens\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class BiLSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_units_forward, hidden_units_backward, num_layers, dropout_rate):\n","        super(BiLSTMModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim) #Embedding layer\n","        self.bilstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_units_forward, num_layers=num_layers, batch_first=True, dropout=dropout_rate, bidirectional=True) #Forward LSTM layer that processes inputs in forward direction\n","        self.bilstm_backward = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_units_backward, num_layers=num_layers, batch_first=True, dropout=dropout_rate, bidirectional=True) #Backward LSTM layer\n","        self.dropout = nn.Dropout(dropout_rate) #Dropout layer\n","        self.fc = nn.Linear((hidden_units_forward + hidden_units_backward) * 2, vocab_size)  #Fully connected layer, *2 is for bidirectional\n","\n","    def forward(self, x):\n","        x = x.long().to(self.embedding.weight.device)  #Ensuring input tensor is on the same device as the model\n","        embedded = self.embedding(x) #Converting input to dense embeddings\n","        lstm_output, _ = self.bilstm(embedded) #Forward output\n","        embedded_reversed = torch.flip(embedded, dims=[1]) #Reversing the embedded input sequences\n","        lstm_output_backward, _ = self.bilstm_backward(embedded_reversed) #Output of the backward LSTM\n","        lstm_output = self.dropout(lstm_output) #Applying dropout\n","        lstm_output_backward = self.dropout(lstm_output_backward)\n","        output = self.fc(torch.cat([lstm_output, lstm_output_backward], dim=-1)) #Getting the final output\n","        return output\n","\n","#Model parameters\n","embedding_dim = 128\n","hidden_units_forward = 128\n","hidden_units_backward = 128\n","num_layers = 2\n","dropout_rate = 0.5\n","\n","model = BiLSTMModel(vocab_size, embedding_dim, hidden_units_forward, hidden_units_backward, num_layers, dropout_rate).to(device)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1715065984852,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"_EyBhuW6nCdi","outputId":"f8d4f9c1-be37-40fd-e9dd-3071a8decb46"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters: 1403524\n"]}],"source":["total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Number of parameters: {total_params}\")"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Training</b></h1>\n"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3967952,"status":"ok","timestamp":1715071036120,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"V3KIE6-mm9gY","outputId":"4534bc2c-332c-48b6-a4dc-45fb8395d2b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Loss: 0.0149\n","Epoch [2/50], Loss: 0.0049\n","Epoch [3/50], Loss: 0.0040\n","Epoch [4/50], Loss: 0.0035\n","Epoch [5/50], Loss: 0.0032\n","Epoch [6/50], Loss: 0.0030\n","Epoch [7/50], Loss: 0.0028\n","Epoch [8/50], Loss: 0.0027\n","Epoch [9/50], Loss: 0.0025\n","Epoch [10/50], Loss: 0.0024\n","Epoch [11/50], Loss: 0.0024\n","Epoch [12/50], Loss: 0.0023\n","Epoch [13/50], Loss: 0.0022\n","Epoch [14/50], Loss: 0.0022\n","Epoch [15/50], Loss: 0.0021\n","Epoch [16/50], Loss: 0.0021\n","Epoch [17/50], Loss: 0.0020\n","Epoch [18/50], Loss: 0.0020\n","Epoch [19/50], Loss: 0.0019\n","Epoch [20/50], Loss: 0.0019\n","Epoch [21/50], Loss: 0.0018\n","Epoch [22/50], Loss: 0.0018\n","Epoch [23/50], Loss: 0.0018\n","Epoch [24/50], Loss: 0.0017\n","Epoch [25/50], Loss: 0.0017\n","Epoch [26/50], Loss: 0.0017\n","Epoch [27/50], Loss: 0.0016\n","Epoch [28/50], Loss: 0.0016\n","Epoch [29/50], Loss: 0.0016\n","Epoch [30/50], Loss: 0.0016\n","Epoch [31/50], Loss: 0.0015\n","Epoch [32/50], Loss: 0.0015\n","Epoch [33/50], Loss: 0.0015\n","Epoch [34/50], Loss: 0.0015\n","Epoch [35/50], Loss: 0.0015\n","Epoch [36/50], Loss: 0.0015\n","Epoch [37/50], Loss: 0.0015\n","Epoch [38/50], Loss: 0.0015\n","Epoch [39/50], Loss: 0.0015\n","Epoch [40/50], Loss: 0.0015\n","Epoch [41/50], Loss: 0.0014\n","Epoch [42/50], Loss: 0.0014\n","Epoch [43/50], Loss: 0.0014\n","Epoch [44/50], Loss: 0.0014\n","Epoch [45/50], Loss: 0.0014\n","Epoch [46/50], Loss: 0.0014\n","Epoch [47/50], Loss: 0.0014\n","Epoch [48/50], Loss: 0.0014\n","Epoch [49/50], Loss: 0.0014\n","Epoch [50/50], Loss: 0.0014\n"]}],"source":["import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Getting the data ready for PyTorch model\n","# We used 16 batch because of computitation resources limitations.\n","train_dataset = TensorDataset(torch.tensor(numerical_train_x), torch.tensor(numerical_train_y))\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# Defining the loss function and optimizer\n","# We chose the CrossEntropyLoss because it gave us the best result\n","# We many different optimizers, and their parameters and this is the best combination that gives the best result.\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=True)\n","\n","#Start of training\n","num_epochs = 50\n","for epoch in range(num_epochs):\n","    model.train() \n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        optimizer.zero_grad()  \n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(inputs)                             # Creation of the output using input\n","        outputs = outputs.view(-1, outputs.shape[-1])       # Reshape outputs\n","        targets = targets.view(-1)                          # Flatten targets\n","        loss = criterion(outputs, targets)                  # Compute the loss using our output and labels(targets)\n","        loss.backward()                                     # Backward pass\n","        optimizer.step()                                    # Update weights using newly found loss\n","        running_loss += loss.item() * inputs.size(0)        # Loss of current input loss \n","    epoch_loss = running_loss / len(train_dataset)          # Total epoch loss\n","    # We printed the losses every epoch because we prefered to see the changes per epoch.\n","    # This could be printed in wider range such as in every (2,3,5,10) epoch, so that it wont spam messages.\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Saving and Loading Weights of the Model</b></h1>\n"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":298,"status":"ok","timestamp":1715071042050,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"54x5YGyLG4Ad"},"outputs":[],"source":["torch.save(model.state_dict(), \"weights/model_weights3.pth\") #Saving the model weights"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1715072173758,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"sUKbwS0DIyrg","outputId":"53281840-72ae-4b34-dc87-9cfa1c78f7d4"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["model = BiLSTMModel(vocab_size, embedding_dim, hidden_units_forward, hidden_units_backward, num_layers, dropout_rate).to(device)\n","\n","#Loading the weights\n","model.load_state_dict(torch.load(\"weights/model_weights3.pth\"))"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Evaluation</b></h1>\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":624,"status":"ok","timestamp":1715066991506,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"HmJyl-mjjdod","outputId":"03266333-0dbe-43c8-d69e-8e74211cb319"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input   :  tarhanerdemyasa yapabilir  idare yetkisiz diyor hukumet bir davranisi yasalarda bir hukum bulup haklari iptal etmekte ve o iptal butun ulkede emsal sayilmaktadir\n","Correct :  tarhanerdemyasa yapabilir  idare yetkisiz diyor hükümet bir davranışı yasalarda bir hüküm bulup hakları iptal etmekte ve o iptal bütün ülkede emsal sayılmaktadır\n","Predicted:  tarhanerdemyasa yapabilir  idare yetkisiz diyor hükümet bir davranışı yasalarda bir hüküm bulup hakları iptal etmekte ve o iptal bütün ülkede emsal sayılmaktadır\n","--------------------------------------------------\n","Input   :  gul  zirve oncesi sunlari soyledi :\n","Correct :  gül  zirve öncesi şunları söyledi :\n","Predicted:  gül  zirve öncesi şunları söyledi :\n","--------------------------------------------------\n","Input   :  fransiz gazetesi  abd nin irak a mudahaleyle ilgili planlarini gozden gecirmek zorunda kaldigini belirterek  bu konuda abd basininda cikan haberlere de atifta bulundu\n","Correct :  fransız gazetesi  abd nin ırak a müdahaleyle ilgili planlarını gözden geçirmek zorunda kaldığını belirterek  bu konuda abd basınında çıkan haberlere de atıfta bulundu\n","Predicted:  fransız gazetesi  abd nin ırak a müdahaleyle ilgili planlarını gözden geçirmek zorunda kaldığını belirterek  bu konuda abd basınında çıkan haberlere de atıfta bulundu\n","--------------------------------------------------\n","Input   :  the pact\n","Correct :  the pact\n","Predicted:  the pact\n","--------------------------------------------------\n","Input   :  aile tedavi gormeli\n","Correct :  aile tedavi görmeli\n","Predicted:  aile tedavi görmeli\n","--------------------------------------------------\n"]}],"source":["import random\n","\n","model.eval()\n","\n","num_examples = 5  #To display 5 random samples from the training set\n","random_indices = random.sample(range(len(numerical_train_x)), num_examples) #Selecting random sentences\n","\n","for i in random_indices:\n","    #Wrong and correct sequences\n","    input_seq = padded_train_x[i]\n","    correct_seq = padded_train_y[i]\n","\n","    #Converting characters to their embeddings\n","    input_indices = [char_to_index[char] for char in input_seq]\n","    correct_indices = [char_to_index[char] for char in correct_seq]\n","\n","    #Removing padding token\n","    input_indices = [index for index in input_indices if index != char_to_index['<PAD>']]\n","    correct_indices = [index for index in correct_indices if index != char_to_index['<PAD>']]\n","\n","    #Decoding numerical indices back into characters by excluding the <EOS> token\n","    input_text = ''.join([index_to_char[index] for index in input_indices if index_to_char[index] != '<EOS>'])\n","    correct_text = ''.join([index_to_char[index] for index in correct_indices if index_to_char[index] != '<EOS>'])\n","\n","    #Preparing input for prediction\n","    input_for_prediction = torch.tensor(numerical_train_x[i]).unsqueeze(0).to(device)\n","\n","    #Predicting the input sequence\n","    with torch.no_grad():\n","        predicted_numerical = model(input_for_prediction) #Output of the model\n","    predicted_seq = [index_to_char[index.item()] for index in predicted_numerical.argmax(dim=-1)[0]] #Converting the output back into the characters\n","\n","    #Exclude everything after the <EOS> token while printing\n","    predicted_text = ''\n","    for char in predicted_seq:\n","        if char == '<EOS>':\n","            break\n","        predicted_text += char\n","\n","    print(\"Input   : \", input_text)\n","    print(\"Correct : \", correct_text)\n","    print(\"Predicted: \", predicted_text)\n","    print(\"-\" * 50)"]},{"cell_type":"markdown","metadata":{},"source":["<h1><b>Testing Model</b></h1>\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Reading test file</h2>\n"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715071049723,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"jYZOMuA82g21"},"outputs":[],"source":["import pandas as pd\n","\n","test_data = pd.read_csv(\"data/test.csv\") #Test data\n","\n","test_sentences = test_data[\"Sentence\"]\n","\n","#Tokenizing test sequences\n","tokenized_test_x = []\n","for sentence in test_sentences:\n","    tokenized_sentence = list(sentence.strip())\n","    tokenized_sentence.append('<EOS>')\n","    tokenized_test_x.append(tokenized_sentence)"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Padding</h2>\n"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":449,"status":"ok","timestamp":1715071052332,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"iEzfTwZb5ITr"},"outputs":[],"source":["max_length_test = max(len(seq) for seq in tokenized_test_x) #Finding the maximum length of sequences in the test data\n","\n","padded_test_x = pad_sequences(tokenized_test_x, maxlen=max_length_test, padding='post', truncating='post', value=PAD_TOKEN, dtype=object) #Padding test sequences until all of them have the length of \"max_length_test\"\n","\n","padded_test_x = [list(seq) for seq in padded_test_x] #Converting back to list"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":544,"status":"ok","timestamp":1715071054726,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"nFL-8Vwa5pWD"},"outputs":[],"source":["#Our training set did not have capitalized letters, but the test set does.\n","#Therefore, we need to make all the capitalized letters lower case first,\n","#And then convert them back to their original form while printing the output file.\n","\n","changed_indices = [] #List to store indices with upper case letters\n","for i, seq in enumerate(padded_test_x):\n","    for j, char in enumerate(seq):\n","        if char.isupper() and char not in ['<EOS>', '<PAD>']: #Excluding the upper case letters in <EOS> and <PAD> tokens\n","            changed_indices.append((i, j))\n","            padded_test_x[i][j] = char.lower() #Converting upper case letters into lower case\n","\n","numerical_test_x = [[char_to_index[char] for char in seq] for seq in padded_test_x]"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Predicting - Saving the Output</h2>\n"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":42354,"status":"ok","timestamp":1715072243271,"user":{"displayName":"Kutay Arda Dinç","userId":"17334735752415627469"},"user_tz":-180},"id":"FpocV-t41y2U"},"outputs":[],"source":["import csv\n","import os\n","\n","output_csv_file = \"eval/results7.csv\" #File to write our final output\n","os.makedirs(os.path.dirname(output_csv_file), exist_ok=True)\n","\n","predictions = [] #List to store predicted sentences\n","\n","for index, row in enumerate(test_data[\"Sentence\"]):\n","    input_sequence = numerical_test_x[index]\n","    input_tensor = torch.tensor(input_sequence).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        predicted_numerical = model(input_tensor)\n","    predicted_sequence = [index_to_char[index.item()] for index in predicted_numerical.argmax(dim=-1)[0]]\n","    if '<EOS>' in predicted_sequence:\n","        eos_index = predicted_sequence.index('<EOS>')\n","        predicted_sequence = predicted_sequence[:eos_index] #Removing anything after the <EOS> token\n","    prediction = ''.join([char for char in predicted_sequence if char != '<EOS>'])\n","    predictions.append(prediction)\n","\n","#Writing predictions to the output file\n","with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['ID', 'Prediction'])\n","    for index, prediction in enumerate(predictions):\n","        # Checking the indices if they corresponds to a changed uppercase letter and if they are, re-capitalize accordingly\n","        for i, j in changed_indices:\n","            if index == i:\n","                prediction = prediction[:j] + prediction[j].upper() + prediction[j+1:]\n","        writer.writerow([index, prediction])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpAa1nrYRLvG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
